{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12cef83e",
   "metadata": {},
   "source": [
    "# Keyword Spotting Model - Homespace Capstone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed064a40",
   "metadata": {},
   "source": [
    "### by ReDay Zarra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4036339b",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b1240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7331fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0985643d",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e665de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255, # Feature Scaling\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batchSize = 16 # Experiment with this\n",
    "size = 64\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    \"data/train\",\n",
    "    target_size = (size, size),\n",
    "    batch_size = batchSize,\n",
    "    class_mode = \"categorical\",\n",
    "    classes = ['Aberto', 'Background Noise', 'Silencio'] \n",
    ")\n",
    "\n",
    "validation_set = test_datagen.flow_from_directory(\n",
    "    \"data/validation\",\n",
    "    target_size = (size, size),\n",
    "    batch_size = batchSize,\n",
    "    class_mode = \"categorical\",\n",
    "    classes = ['Aberto', 'Background Noise', 'Silencio']\n",
    ")\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    \"data/test\",\n",
    "    target_size = (size, size),\n",
    "    batch_size = batchSize,\n",
    "    class_mode = \"categorical\",\n",
    "    classes = ['Aberto', 'Background Noise', 'Silencio'], \n",
    "    shuffle=False # Set shuffle to False to keep the order of test images for evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc4c9f",
   "metadata": {},
   "source": [
    "## Building a Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a092dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc320da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to build a model with varying parameters\n",
    "def build_model(num_filters, kernel_size, dropout_rate):\n",
    "    model = Sequential([\n",
    "        Conv2D(num_filters, kernel_size, activation='relu', input_shape=(size, size, 3)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define a function to compile, train, and evaluate a model\n",
    "def train_and_evaluate_model(model, optimizer, learning_rate):\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(\n",
    "        training_set,\n",
    "        steps_per_epoch=len(training_set),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_set,\n",
    "        validation_steps=len(validation_set)\n",
    "    )\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(test_set, steps=len(test_set))\n",
    "    return test_accuracy\n",
    "\n",
    "epochs = 25  # Adjust this value based on the convergence of your model\n",
    "\n",
    "# Iterate over different model architectures and hyperparameters\n",
    "best_accuracy = 0\n",
    "best_parameters = None\n",
    "for num_filters in [16, 32, 64]:\n",
    "    for kernel_size in [(3, 3), (5, 5)]:\n",
    "        for dropout_rate in [0.25, 0.5, 0.75]:\n",
    "            for optimizer in [tf.keras.optimizers.Adam, tf.keras.optimizers.RMSprop]:\n",
    "                for learning_rate in [0.001, 0.01]:\n",
    "                    model = build_model(num_filters, kernel_size, dropout_rate)\n",
    "                    accuracy = train_and_evaluate_model(model, optimizer, learning_rate)\n",
    "                    \n",
    "                    if accuracy > best_accuracy:\n",
    "                        best_accuracy = accuracy\n",
    "                        best_parameters = (num_filters, kernel_size, dropout_rate, optimizer, learning_rate)\n",
    "\n",
    "# Train the best model with the best parameters found above\n",
    "best_num_filters, best_kernel_size, best_dropout_rate, best_optimizer, best_learning_rate = best_parameters\n",
    "final_model = build_model(best_num_filters, best_kernel_size, best_dropout_rate)\n",
    "\n",
    "# Compile the final model\n",
    "final_model.compile(optimizer=best_optimizer(learning_rate=best_learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02e8cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model\n",
    "final_history = final_model.fit(\n",
    "    training_set,\n",
    "    steps_per_epoch=len(training_set),\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_set,\n",
    "    validation_steps=len(validation_set)\n",
    ")\n",
    "\n",
    "# Evaluate the final model\n",
    "final_test_loss, final_test_accuracy = final_model.evaluate(test_set, steps=len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa9d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "final_model.save('my_custom_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cacce7c",
   "metadata": {},
   "source": [
    "## Using the VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e441c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Data Generator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batchSize = 16\n",
    "size = 224\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    \"data/train\",\n",
    "    target_size=(size, size),\n",
    "    batch_size=batchSize,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "validation_set = test_datagen.flow_from_directory(\n",
    "    \"data/validation\",\n",
    "    target_size=(size, size),\n",
    "    batch_size=batchSize,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    \"data/test\",\n",
    "    target_size=(size, size),\n",
    "    batch_size=batchSize,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9210f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(size, size, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pretrained_model(base_model, dropout_rate):\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1667fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = build_pretrained_model(base_model, dropout_rate=0.5)\n",
    "pretrained_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "pretrained_history = pretrained_model.fit(\n",
    "    training_set,\n",
    "    steps_per_epoch=len(training_set),\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_set,\n",
    "    validation_steps=len(validation_set)\n",
    ")\n",
    "\n",
    "pretrained_test_loss, pretrained_test_accuracy = pretrained_model.evaluate(test_set, steps=len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a369c6e",
   "metadata": {},
   "source": [
    "## Using the EfficientNetB0 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2998dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: efficientnet in c:\\users\\reday\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in c:\\users\\reday\\anaconda3\\lib\\site-packages (from efficientnet) (1.0.8)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\reday\\anaconda3\\lib\\site-packages (from efficientnet) (0.19.2)\n",
      "Requirement already satisfied: h5py in c:\\users\\reday\\anaconda3\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\reday\\anaconda3\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.24.2)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\reday\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (2.19.3)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\reday\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (2.8.4)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\reday\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (1.9.1)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in c:\\users\\reday\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (9.2.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\reday\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (2021.7.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\reday\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (1.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\reday\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\reday\\anaconda3\\lib\\site-packages (from packaging>=20.0->scikit-image->efficientnet) (3.0.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4405f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from efficientnet.keras import EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9f12f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 168 images belonging to 3 classes.\n",
      "Found 36 images belonging to 3 classes.\n",
      "Found 36 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Image Data Generator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batchSize = 16\n",
    "size = 224\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    \"data/train\",\n",
    "    target_size=(size, size),\n",
    "    batch_size=batchSize,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "validation_set = test_datagen.flow_from_directory(\n",
    "    \"data/validation\",\n",
    "    target_size=(size, size),\n",
    "    batch_size=batchSize,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    \"data/test\",\n",
    "    target_size=(size, size),\n",
    "    batch_size=batchSize,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6674190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained EfficientNetB0 model without the top layers\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(size, size, 3))\n",
    "\n",
    "# Freeze the layers of the pre-trained model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4d7dbe8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "11/11 [==============================] - 10s 526ms/step - loss: 1.0183 - accuracy: 0.4940 - val_loss: 0.7533 - val_accuracy: 0.6389\n",
      "Epoch 2/30\n",
      "11/11 [==============================] - 4s 401ms/step - loss: 0.6545 - accuracy: 0.7083 - val_loss: 0.4998 - val_accuracy: 0.7778\n",
      "Epoch 3/30\n",
      "11/11 [==============================] - 5s 413ms/step - loss: 0.5417 - accuracy: 0.7381 - val_loss: 0.4822 - val_accuracy: 0.7778\n",
      "Epoch 4/30\n",
      "11/11 [==============================] - 5s 410ms/step - loss: 0.4881 - accuracy: 0.7798 - val_loss: 0.4209 - val_accuracy: 0.8056\n",
      "Epoch 5/30\n",
      "11/11 [==============================] - 4s 396ms/step - loss: 0.5059 - accuracy: 0.7857 - val_loss: 0.3929 - val_accuracy: 0.8056\n",
      "Epoch 6/30\n",
      "11/11 [==============================] - 4s 400ms/step - loss: 0.3906 - accuracy: 0.8214 - val_loss: 0.3650 - val_accuracy: 0.8056\n",
      "Epoch 7/30\n",
      "11/11 [==============================] - 5s 433ms/step - loss: 0.3944 - accuracy: 0.8393 - val_loss: 0.3508 - val_accuracy: 0.7778\n",
      "Epoch 8/30\n",
      "11/11 [==============================] - 5s 409ms/step - loss: 0.3637 - accuracy: 0.8155 - val_loss: 0.3397 - val_accuracy: 0.8611\n",
      "Epoch 9/30\n",
      "11/11 [==============================] - 5s 404ms/step - loss: 0.3023 - accuracy: 0.8750 - val_loss: 0.3266 - val_accuracy: 0.8611\n",
      "Epoch 10/30\n",
      "11/11 [==============================] - 5s 410ms/step - loss: 0.2826 - accuracy: 0.8810 - val_loss: 0.3147 - val_accuracy: 0.8611\n",
      "Epoch 11/30\n",
      "11/11 [==============================] - 5s 413ms/step - loss: 0.2368 - accuracy: 0.9345 - val_loss: 0.3025 - val_accuracy: 0.8611\n",
      "Epoch 12/30\n",
      "11/11 [==============================] - 4s 401ms/step - loss: 0.3252 - accuracy: 0.8452 - val_loss: 0.3217 - val_accuracy: 0.8611\n",
      "Epoch 13/30\n",
      "11/11 [==============================] - 5s 405ms/step - loss: 0.2674 - accuracy: 0.9107 - val_loss: 0.3029 - val_accuracy: 0.8889\n",
      "Epoch 14/30\n",
      "11/11 [==============================] - 5s 421ms/step - loss: 0.2478 - accuracy: 0.8988 - val_loss: 0.2923 - val_accuracy: 0.8889\n",
      "Epoch 15/30\n",
      "11/11 [==============================] - 5s 404ms/step - loss: 0.2407 - accuracy: 0.8929 - val_loss: 0.2751 - val_accuracy: 0.8889\n",
      "Epoch 16/30\n",
      "11/11 [==============================] - 5s 408ms/step - loss: 0.2239 - accuracy: 0.9167 - val_loss: 0.2779 - val_accuracy: 0.8889\n",
      "Epoch 17/30\n",
      "11/11 [==============================] - 5s 432ms/step - loss: 0.2599 - accuracy: 0.8810 - val_loss: 0.2749 - val_accuracy: 0.8889\n",
      "Epoch 18/30\n",
      "11/11 [==============================] - 5s 431ms/step - loss: 0.2131 - accuracy: 0.9167 - val_loss: 0.2664 - val_accuracy: 0.8611\n",
      "Epoch 19/30\n",
      "11/11 [==============================] - 5s 406ms/step - loss: 0.2656 - accuracy: 0.8929 - val_loss: 0.2573 - val_accuracy: 0.9167\n",
      "Epoch 20/30\n",
      "11/11 [==============================] - 5s 417ms/step - loss: 0.2440 - accuracy: 0.9048 - val_loss: 0.2554 - val_accuracy: 0.8889\n",
      "Epoch 21/30\n",
      "11/11 [==============================] - 5s 425ms/step - loss: 0.2381 - accuracy: 0.8929 - val_loss: 0.2469 - val_accuracy: 0.9167\n",
      "Epoch 22/30\n",
      "11/11 [==============================] - 5s 408ms/step - loss: 0.2077 - accuracy: 0.9524 - val_loss: 0.2540 - val_accuracy: 0.9444\n",
      "Epoch 23/30\n",
      "11/11 [==============================] - 4s 402ms/step - loss: 0.1869 - accuracy: 0.9464 - val_loss: 0.2412 - val_accuracy: 0.8611\n",
      "Epoch 24/30\n",
      "11/11 [==============================] - 5s 409ms/step - loss: 0.2391 - accuracy: 0.9048 - val_loss: 0.2346 - val_accuracy: 0.9444\n",
      "Epoch 25/30\n",
      "11/11 [==============================] - 5s 414ms/step - loss: 0.1955 - accuracy: 0.9167 - val_loss: 0.2368 - val_accuracy: 0.8611\n",
      "Epoch 26/30\n",
      "11/11 [==============================] - 5s 406ms/step - loss: 0.1741 - accuracy: 0.9345 - val_loss: 0.2333 - val_accuracy: 0.8611\n",
      "Epoch 27/30\n",
      "11/11 [==============================] - 5s 404ms/step - loss: 0.1547 - accuracy: 0.9464 - val_loss: 0.2850 - val_accuracy: 0.8611\n",
      "Epoch 28/30\n",
      "11/11 [==============================] - 5s 417ms/step - loss: 0.1858 - accuracy: 0.9286 - val_loss: 0.2462 - val_accuracy: 0.8611\n",
      "Epoch 29/30\n",
      "11/11 [==============================] - 5s 404ms/step - loss: 0.1800 - accuracy: 0.9345 - val_loss: 0.2395 - val_accuracy: 0.8611\n",
      "Epoch 30/30\n",
      "11/11 [==============================] - 4s 400ms/step - loss: 0.1633 - accuracy: 0.9226 - val_loss: 0.2573 - val_accuracy: 0.9167\n",
      "3/3 [==============================] - 1s 201ms/step - loss: 0.3598 - accuracy: 0.8889\n"
     ]
    }
   ],
   "source": [
    "# Create a new model that uses the pre-trained model as its base\n",
    "def build_pretrained_model(base_model, dropout_rate, number_of_classes):\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(number_of_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the number of classes for your specific problem\n",
    "number_of_classes = 3\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# Train and evaluate the new model\n",
    "pretrained_model = build_pretrained_model(base_model, dropout_rate, number_of_classes)\n",
    "pretrained_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "pretrained_history = pretrained_model.fit(\n",
    "    training_set,\n",
    "    steps_per_epoch=len(training_set),\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_set,\n",
    "    validation_steps=len(validation_set)\n",
    ")\n",
    "\n",
    "pretrained_test_loss, pretrained_test_accuracy = pretrained_model.evaluate(test_set, steps=len(test_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c6c151",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28c45d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_model_optimization in c:\\users\\reday\\appdata\\roaming\\python\\python39\\site-packages (0.7.4)\n",
      "Requirement already satisfied: absl-py~=1.2 in c:\\users\\reday\\anaconda3\\lib\\site-packages (from tensorflow_model_optimization) (1.3.0)\n",
      "Requirement already satisfied: numpy~=1.23 in c:\\users\\reday\\anaconda3\\lib\\site-packages (from tensorflow_model_optimization) (1.24.2)\n",
      "Requirement already satisfied: six~=1.14 in c:\\users\\reday\\anaconda3\\lib\\site-packages (from tensorflow_model_optimization) (1.16.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in c:\\users\\reday\\anaconda3\\lib\\site-packages (from tensorflow_model_optimization) (0.1.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user tensorflow_model_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81db404d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Quantizing a tf.keras Model inside another tf.keras Model is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21832\\2106839336.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Quantize the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mq_aware_pretrained_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquantize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m q_aware_pretrained_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n\u001b[0;32m      8\u001b[0m                                   loss='categorical_crossentropy', metrics=['accuracy'])\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_model_optimization\\python\\core\\quantization\\keras\\quantize.py\u001b[0m in \u001b[0;36mquantize_model\u001b[1;34m(to_quantize, quantized_layer_name_prefix)\u001b[0m\n\u001b[0;32m    144\u001b[0m         'Functional model.')\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m   \u001b[0mannotated_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquantize_annotate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_quantize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m   return quantize_apply(\n\u001b[0;32m    148\u001b[0m       annotated_model, quantized_layer_name_prefix=quantized_layer_name_prefix)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_model_optimization\\python\\core\\quantization\\keras\\quantize.py\u001b[0m in \u001b[0;36mquantize_annotate_model\u001b[1;34m(to_annotate)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mquantize_annotate_mod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQuantizeAnnotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m   return keras.models.clone_model(\n\u001b[0m\u001b[0;32m    232\u001b[0m       to_annotate, input_tensors=None, clone_function=_add_quant_wrapper)\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\models\\cloning.py\u001b[0m in \u001b[0;36mclone_model\u001b[1;34m(model, input_tensors, clone_function)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m             return _clone_sequential_model(\n\u001b[0m\u001b[0;32m    503\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclone_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\models\\cloning.py\u001b[0m in \u001b[0;36m_clone_sequential_model\u001b[1;34m(model, input_tensors, layer_fn)\u001b[0m\n\u001b[0;32m    361\u001b[0m             \u001b[0m_clone_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m             \u001b[1;32melse\u001b[0m \u001b[0mlayer_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         )\n\u001b[0;32m    365\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcloned_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_model_optimization\\python\\core\\quantization\\keras\\quantize.py\u001b[0m in \u001b[0;36m_add_quant_wrapper\u001b[1;34m(layer)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m       raise ValueError(\n\u001b[0m\u001b[0;32m    226\u001b[0m           \u001b[1;34m'Quantizing a tf.keras Model inside another tf.keras Model is not supported.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m       )\n",
      "\u001b[1;31mValueError\u001b[0m: Quantizing a tf.keras Model inside another tf.keras Model is not supported."
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# Quantize the model\n",
    "q_aware_pretrained_model = quantize_model(pretrained_model)\n",
    "q_aware_pretrained_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the quantized model\n",
    "q_aware_pretrained_history = q_aware_pretrained_model.fit(\n",
    "    training_set,\n",
    "    steps_per_epoch=len(training_set),\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_set,\n",
    "    validation_steps=len(validation_set)\n",
    ")\n",
    "\n",
    "# Evaluate the quantized model\n",
    "q_aware_pretrained_test_loss, q_aware_pretrained_test_accuracy = q_aware_pretrained_model.evaluate(\n",
    "    test_set, steps=len(test_set)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877aa930",
   "metadata": {},
   "source": [
    "## Convert to TensorFlow Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a7d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create a temporary directory\n",
    "tempdir = tempfile.mkdtemp()\n",
    "\n",
    "# Save the quantized model to the temporary directory\n",
    "quantized_model_file = os.path.join(tempdir, 'quantized_model.tflite')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_pretrained_model)\n",
    "tflite_quantized_model = converter.convert()\n",
    "\n",
    "# Save the quantized model as a .tflite file\n",
    "with open(quantized_model_file, 'wb') as f:\n",
    "    f.write(tflite_quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6612b98",
   "metadata": {},
   "source": [
    "## Convert to .CC File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63040a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt-get install xxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883bb98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxd -i quantized_model.tflite > quantized_model.cc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
